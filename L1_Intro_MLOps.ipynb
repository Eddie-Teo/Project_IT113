{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63641a3-c74d-42b5-a51b-0297699c352d",
   "metadata": {},
   "source": [
    "# Introduction to MLOps in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a348c-1988-4786-aceb-5b9058a6bbad",
   "metadata": {},
   "source": [
    "Run the following exercises and explore the questions asked.\n",
    "\n",
    "**Assumptions**: Participants have basic familiarity with Python, Pandas, Scikit-learn, and Jupyter Notebook usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613834d6-2e1f-4a52-88aa-e404f5cbf1d6",
   "metadata": {},
   "source": [
    "## Exercise 1: Reproducibility\n",
    "### Sample A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa6e86a-913d-4b2b-bf51-958666821d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split with test_size=0.2, random_state=321\n",
      "Data scaled using StandardScaler\n",
      "Trained KNeighborsClassifier with n_neighbors=5\n",
      "\n",
      "--- Results ---\n",
      "Parameters: test_size=0.2, random_state=321, n_neighbors=5\n",
      "Achieved Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "import random\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321, stratify=y)\n",
    "print(f\"Data split with test_size=0.2, random_state=321\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Data scaled using StandardScaler\")\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(f\"Trained KNeighborsClassifier with n_neighbors=5\")\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Results ---\")\n",
    "print(f\"Parameters: test_size=0.2, random_state=321, n_neighbors=5\")\n",
    "print(f\"Achieved Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149a114-0657-418d-be74-4a1c772d8cc3",
   "metadata": {},
   "source": [
    "### *Questions*:\n",
    "1. Run code sample A. Note the accuracy.\n",
    "2. Change `test_size` to 0.2 and re-run all cells. How easy was it? What's the new accuracy?\n",
    "3. How would you systematically try 5 different `random_state` values (e.g., 0, 42, 100, 123, 2024)?\n",
    "4. How do you reliably track which parameters (`random_state`, `test_size`, `n_neighbors`) produced the best score?\n",
    "5. Could a teammate easily understand and run this notebook to get the exact same result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4489a-e6c6-4ce3-b69b-5dd76054144e",
   "metadata": {},
   "source": [
    "### Sample B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5409529d-a296-4b0e-9364-542edcf5491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration ---\n",
      "data_source: sklearn_iris\n",
      "test_size: 0.3\n",
      "random_state: 123\n",
      "stratify: True\n",
      "model_params: {'n_neighbors': 5}\n",
      "features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "target: target\n",
      "\n",
      "Loading data from: sklearn_iris\n",
      "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target: target\n",
      "Data shape: (150, 4), Target shape: (150,)\n",
      "\n",
      "Splitting data: test_size=0.3, random_state=123, stratify=True\n",
      "Train shapes: X=(105, 4), y=(105,)\n",
      "Test shapes: X=(45, 4), y=(45,)\n",
      "\n",
      "Preprocessing data using StandardScaler\n",
      "Scaling complete.\n",
      "\n",
      "Training KNeighborsClassifier with params: {'n_neighbors': 5}\n",
      "Training complete.\n",
      "\n",
      "Evaluating model\n",
      "Evaluation complete.\n",
      "\n",
      "--- Results ---\n",
      "Configuration Used: {'data_source': 'sklearn_iris', 'test_size': 0.3, 'random_state': 123, 'stratify': True, 'model_params': {'n_neighbors': 5}, 'features': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'target': 'target'}\n",
      "Achieved Accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from typing import Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "CONFIG: Dict[str, Any] = {\n",
    "    \"data_source\": \"sklearn_iris\",\n",
    "    \"test_size\": 0.3,\n",
    "    \"random_state\": 123,\n",
    "    \"stratify\": True,\n",
    "    \"model_params\": {\n",
    "        \"n_neighbors\": 5,\n",
    "    },\n",
    "    \"features\": ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], # Explicit feature names\n",
    "    \"target\": 'target'\n",
    "}\n",
    "\n",
    "print(\"--- Configuration ---\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_data(source: str, feature_cols: list, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Loads data based on the source specified in config.\"\"\"\n",
    "    print(f\"\\nLoading data from: {source}\")\n",
    "    if source == \"sklearn_iris\":\n",
    "        iris = load_iris()\n",
    "        df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                          columns=feature_cols + [target_col])\n",
    "        # Convert target to int if needed (already is for iris)\n",
    "        df[target_col] = df[target_col].astype(int)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data source: {source}\")\n",
    "\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    print(f\"Features: {list(X.columns)}\")\n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"Data shape: {X.shape}, Target shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "def split_data(X: pd.DataFrame, y: pd.Series, test_size: float, random_state: int, stratify: bool) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Splits data into training and testing sets.\"\"\"\n",
    "    print(f\"\\nSplitting data: test_size={test_size}, random_state={random_state}, stratify={stratify}\")\n",
    "    stratify_col = y if stratify else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_col\n",
    "    )\n",
    "    print(f\"Train shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Test shapes: X={X_test.shape}, y={y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def preprocess_data(X_train: pd.DataFrame, X_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, StandardScaler]:\n",
    "    \"\"\"Scales the feature data using StandardScaler.\"\"\"\n",
    "    print(\"\\nPreprocessing data using StandardScaler\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Scaling complete.\")\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def train_model(X_train: np.ndarray, y_train: pd.Series, model_params: Dict[str, Any]) -> KNeighborsClassifier:\n",
    "    \"\"\"Trains a KNeighborsClassifier model.\"\"\"\n",
    "    print(f\"\\nTraining KNeighborsClassifier with params: {model_params}\")\n",
    "    model = KNeighborsClassifier(**model_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model: KNeighborsClassifier, X_test: np.ndarray, y_test: pd.Series) -> float:\n",
    "    \"\"\"Evaluates the model and returns accuracy.\"\"\"\n",
    "    print(\"\\nEvaluating model\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Evaluation complete.\")\n",
    "    return acc\n",
    "\n",
    "# --- Main Workflow ---\n",
    "\n",
    "X, y = load_data(CONFIG[\"data_source\"], CONFIG[\"features\"], CONFIG[\"target\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(\n",
    "    X, y,\n",
    "    test_size=CONFIG[\"test_size\"],\n",
    "    random_state=CONFIG[\"random_state\"],\n",
    "    stratify=CONFIG[\"stratify\"]\n",
    ")\n",
    "\n",
    "X_train_scaled, X_test_scaled, scaler_object = preprocess_data(X_train, X_test)\n",
    "\n",
    "trained_model = train_model(X_train_scaled, y_train, CONFIG[\"model_params\"])\n",
    "\n",
    "accuracy = evaluate_model(trained_model, X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\n--- Results ---\")\n",
    "print(f\"Configuration Used: {CONFIG}\")\n",
    "print(f\"Achieved Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc85c96-4a72-4bb0-9fe4-77257593078a",
   "metadata": {},
   "source": [
    "### *Questions*:\n",
    "\n",
    "1. Revisit the questions for code sample A.\n",
    "2. How easy is it to change `test_size` or `random_state` now?\n",
    "3. How would you systematically try 5 different `random_state` values?\n",
    "4. How do you reliably track which parameters produced the best score?\n",
    "5. What problems still remain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ace64-208c-4839-9565-8c1f89cd9aa8",
   "metadata": {},
   "source": [
    "## Exercise 2: Tracking Experiments\n",
    "### Sample C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f3b1954-9679-4135-852b-cad029a6aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setup: Loading Libraries ---\n",
      "Pandas: 2.2.2\n",
      "Scikit-learn: 1.5.1\n",
      "Joblib: 1.4.2\n",
      "\n",
      "--- Base Configuration ---\n",
      "data_source: sklearn_iris\n",
      "test_size: 0.3\n",
      "random_state: 42\n",
      "stratify: True\n",
      "features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "target: target\n",
      "artifact_dir: manual_models\n",
      "\n",
      "Created directory for saving models: manual_models\n",
      "\n",
      "--- Defining Experiment Runs ---\n",
      "Defined 7 experiment runs.\n",
      "\n",
      "--- Starting Experiment Tracking Loop ---\n",
      "Saved scaler to manual_models\\scaler_rs_42.joblib\n",
      "\n",
      "--- Running Experiment 1/7 ---\n",
      "Parameters: {'model_type': 'DecisionTree', 'max_depth': 2, 'criterion': 'gini'}\n",
      "\n",
      "Training DecisionTreeClassifier with params: {'model_type': 'DecisionTree', 'max_depth': 2, 'criterion': 'gini'}\n",
      "\n",
      "Evaluating model\n",
      "Metrics: {'accuracy': 0.8888888888888888}\n",
      "Saved model artifact to: manual_models\\model_run_1_depth_2_crit_gini_acc_0_889.joblib\n",
      "\n",
      "--- Running Experiment 2/7 ---\n",
      "Parameters: {'model_type': 'DecisionTree', 'max_depth': 3, 'criterion': 'gini'}\n",
      "\n",
      "Training DecisionTreeClassifier with params: {'model_type': 'DecisionTree', 'max_depth': 3, 'criterion': 'gini'}\n",
      "\n",
      "Evaluating model\n",
      "Metrics: {'accuracy': 0.9777777777777777}\n",
      "Saved model artifact to: manual_models\\model_run_2_depth_3_crit_gini_acc_0_978.joblib\n",
      "\n",
      "--- Running Experiment 3/7 ---\n",
      "Parameters: {'model_type': 'DecisionTree', 'max_depth': 4, 'criterion': 'gini'}\n",
      "\n",
      "Training DecisionTreeClassifier with params: {'model_type': 'DecisionTree', 'max_depth': 4, 'criterion': 'gini'}\n",
      "\n",
      "Evaluating model\n",
      "Metrics: {'accuracy': 0.8888888888888888}\n",
      "Saved model artifact to: manual_models\\model_run_3_depth_4_crit_gini_acc_0_889.joblib\n",
      "\n",
      "--- Running Experiment 4/7 ---\n",
      "Parameters: {'model_type': 'DecisionTree', 'max_depth': 5, 'criterion': 'gini'}\n",
      "\n",
      "Training DecisionTreeClassifier with params: {'model_type': 'DecisionTree', 'max_depth': 5, 'criterion': 'gini'}\n",
      "\n",
      "Evaluating model\n",
      "Metrics: {'accuracy': 0.9111111111111111}\n",
      "Saved model artifact to: manual_models\\model_run_4_depth_5_crit_gini_acc_0_911.joblib\n",
      "\n",
      "--- Running Experiment 5/7 ---\n",
      "Parameters: {'model_type': 'DecisionTree', 'max_depth': None, 'criterion': 'gini'}\n",
      "\n",
      "Training DecisionTreeClassifier with params: {'model_type': 'DecisionTree', 'max_depth': None, 'criterion': 'gini'}\n",
      "\n",
      "Evaluating model\n",
      "Metrics: {'accuracy': 0.9111111111111111}\n",
      "Saved model artifact to: manual_models\\model_run_5_depth_None_crit_gini_acc_0_911.joblib\n",
      "\n",
      "--- Running Experiment 6/7 ---\n",
      "Parameters: {'model_type': 'DecisionTree', 'max_depth': 3, 'criterion': 'entropy'}\n",
      "\n",
      "Training DecisionTreeClassifier with params: {'model_type': 'DecisionTree', 'max_depth': 3, 'criterion': 'entropy'}\n",
      "\n",
      "Evaluating model\n",
      "Metrics: {'accuracy': 0.9333333333333333}\n",
      "Saved model artifact to: manual_models\\model_run_6_depth_3_crit_entropy_acc_0_933.joblib\n",
      "\n",
      "--- Running Experiment 7/7 ---\n",
      "Parameters: {'model_type': 'DecisionTree', 'max_depth': 5, 'criterion': 'entropy'}\n",
      "\n",
      "Training DecisionTreeClassifier with params: {'model_type': 'DecisionTree', 'max_depth': 5, 'criterion': 'entropy'}\n",
      "\n",
      "Evaluating model\n",
      "Metrics: {'accuracy': 0.8888888888888888}\n",
      "Saved model artifact to: manual_models\\model_run_7_depth_5_crit_entropy_acc_0_889.joblib\n",
      "\n",
      "--- Experiment Loop Finished ---\n",
      "\n",
      "--- Analyzing Results ---\n",
      "Results Summary DataFrame:\n",
      "   run_id    model_type  max_depth criterion  accuracy                                     model_artifact\n",
      "0       1  DecisionTree        2.0      gini  0.888889  manual_models\\model_run_1_depth_2_crit_gini_ac...\n",
      "1       2  DecisionTree        3.0      gini  0.977778  manual_models\\model_run_2_depth_3_crit_gini_ac...\n",
      "2       3  DecisionTree        4.0      gini  0.888889  manual_models\\model_run_3_depth_4_crit_gini_ac...\n",
      "3       4  DecisionTree        5.0      gini  0.911111  manual_models\\model_run_4_depth_5_crit_gini_ac...\n",
      "4       5  DecisionTree        NaN      gini  0.911111  manual_models\\model_run_5_depth_None_crit_gini...\n",
      "5       6  DecisionTree        3.0   entropy  0.933333  manual_models\\model_run_6_depth_3_crit_entropy...\n",
      "6       7  DecisionTree        5.0   entropy  0.888889  manual_models\\model_run_7_depth_5_crit_entropy...\n",
      "\n",
      "Results Sorted by Accuracy (Descending):\n",
      "   run_id    model_type  max_depth criterion  accuracy                                     model_artifact\n",
      "1       2  DecisionTree        3.0      gini  0.977778  manual_models\\model_run_2_depth_3_crit_gini_ac...\n",
      "5       6  DecisionTree        3.0   entropy  0.933333  manual_models\\model_run_6_depth_3_crit_entropy...\n",
      "3       4  DecisionTree        5.0      gini  0.911111  manual_models\\model_run_4_depth_5_crit_gini_ac...\n",
      "4       5  DecisionTree        NaN      gini  0.911111  manual_models\\model_run_5_depth_None_crit_gini...\n",
      "0       1  DecisionTree        2.0      gini  0.888889  manual_models\\model_run_1_depth_2_crit_gini_ac...\n",
      "2       3  DecisionTree        4.0      gini  0.888889  manual_models\\model_run_3_depth_4_crit_gini_ac...\n",
      "6       7  DecisionTree        5.0   entropy  0.888889  manual_models\\model_run_7_depth_5_crit_entropy...\n",
      "\n",
      "--- Best Performing Run ---\n",
      "Run ID: 2\n",
      "Parameters: {'model_type': 'DecisionTree', 'max_depth': 3.0, 'criterion': 'gini'}\n",
      "Accuracy: 0.9778\n",
      "Model Artifact: manual_models\\model_run_2_depth_3_crit_gini_acc_0_978.joblib\n",
      "\n",
      "--- Listing Saved Artifacts ---\n",
      "Files in 'manual_models':\n",
      "- model_run_1_depth_2_crit_gini_acc_0_889.joblib\n",
      "- model_run_2_depth_3_crit_gini_acc_0_978.joblib\n",
      "- model_run_3_depth_4_crit_gini_acc_0_889.joblib\n",
      "- model_run_4_depth_5_crit_gini_acc_0_911.joblib\n",
      "- model_run_5_depth_None_crit_gini_acc_0_911.joblib\n",
      "- model_run_6_depth_3_crit_entropy_acc_0_933.joblib\n",
      "- model_run_7_depth_5_crit_entropy_acc_0_889.joblib\n",
      "- scaler_rs_42.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from typing import Dict, Any, Tuple, List\n",
    "import numpy as np\n",
    "import joblib # For saving models (artifacts)\n",
    "import os # For interacting with the file system\n",
    "import time # To make filenames unique if runs are fast\n",
    "\n",
    "print(\"--- Setup: Loading Libraries ---\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "import sklearn\n",
    "print(f\"Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"Joblib: {joblib.__version__}\")\n",
    "\n",
    "# Define base settings, hyperparameters\n",
    "BASE_CONFIG: Dict[str, Any] = {\n",
    "    \"data_source\": \"sklearn_iris\",\n",
    "    \"test_size\": 0.3,\n",
    "    \"random_state\": 42, # Using a fixed state for splitting consistency across runs\n",
    "    \"stratify\": True,\n",
    "    \"features\": ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'],\n",
    "    \"target\": 'target',\n",
    "    \"artifact_dir\": \"manual_models\" # Directory to save models\n",
    "}\n",
    "\n",
    "print(\"\\n--- Base Configuration ---\")\n",
    "for key, value in BASE_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create artifact directory if it doesn't exist\n",
    "if not os.path.exists(BASE_CONFIG[\"artifact_dir\"]):\n",
    "    os.makedirs(BASE_CONFIG[\"artifact_dir\"])\n",
    "    print(f\"\\nCreated directory for saving models: {BASE_CONFIG['artifact_dir']}\")\n",
    "\n",
    "def load_data(source: str, feature_cols: list, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Loads data based on the source specified in config.\"\"\"\n",
    "    # print(f\"\\nLoading data from: {source}\") # Reduced verbosity for loop\n",
    "    if source == \"sklearn_iris\":\n",
    "        iris = load_iris()\n",
    "        df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                          columns=feature_cols + [target_col])\n",
    "        df[target_col] = df[target_col].astype(int)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data source: {source}\")\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    return X, y\n",
    "\n",
    "def split_data(X: pd.DataFrame, y: pd.Series, test_size: float, random_state: int, stratify: bool) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Splits data into training and testing sets.\"\"\"\n",
    "    # print(f\"\\nSplitting data: test_size={test_size}, random_state={random_state}, stratify={stratify}\")\n",
    "    stratify_col = y if stratify else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_col\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def preprocess_data(X_train: pd.DataFrame, X_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, StandardScaler]:\n",
    "    \"\"\"Scales the feature data using StandardScaler.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "# Updated train_model function for Decision Tree\n",
    "def train_model(X_train: np.ndarray, y_train: pd.Series, model_params: Dict[str, Any]) -> DecisionTreeClassifier:\n",
    "    \"\"\"Trains a DecisionTreeClassifier model.\"\"\"\n",
    "    print(f\"\\nTraining DecisionTreeClassifier with params: {model_params}\")\n",
    "    # Ensure only valid DT parameters are passed\n",
    "    valid_dt_params = {k: v for k, v in model_params.items() if k in DecisionTreeClassifier().get_params()}\n",
    "    model = DecisionTreeClassifier(**valid_dt_params, random_state=BASE_CONFIG[\"random_state\"]) # Add base random state for tree consistency\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model: DecisionTreeClassifier, X_test: np.ndarray, y_test: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"Evaluates the model and returns a dictionary of metrics (e.g., accuracy).\"\"\"\n",
    "    print(\"\\nEvaluating model\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    # Could add more metrics here (precision, recall, f1, etc.)\n",
    "    metrics = {\"accuracy\": acc}\n",
    "    return metrics\n",
    "\n",
    "# --- Experiment Setup ---\n",
    "print(\"\\n--- Defining Experiment Runs ---\")\n",
    "\n",
    "# Define parameter sets to try\n",
    "experiment_params: List[Dict[str, Any]] = [\n",
    "    {\"model_type\": \"DecisionTree\", \"max_depth\": 2, \"criterion\": \"gini\"},\n",
    "    {\"model_type\": \"DecisionTree\", \"max_depth\": 3, \"criterion\": \"gini\"},\n",
    "    {\"model_type\": \"DecisionTree\", \"max_depth\": 4, \"criterion\": \"gini\"},\n",
    "    {\"model_type\": \"DecisionTree\", \"max_depth\": 5, \"criterion\": \"gini\"},\n",
    "    {\"model_type\": \"DecisionTree\", \"max_depth\": None, \"criterion\": \"gini\"}, # None = no limit\n",
    "    {\"model_type\": \"DecisionTree\", \"max_depth\": 3, \"criterion\": \"entropy\"},\n",
    "    {\"model_type\": \"DecisionTree\", \"max_depth\": 5, \"criterion\": \"entropy\"},\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(experiment_params)} experiment runs.\")\n",
    "\n",
    "# List to store results from each run\n",
    "results_list: List[Dict[str, Any]] = []\n",
    "\n",
    "# --- Execute Experiment Loop ---\n",
    "print(\"\\n--- Starting Experiment Tracking Loop ---\")\n",
    "\n",
    "# 1. Load and Split Data (do this once outside the loop if splitting is consistent)\n",
    "X, y = load_data(BASE_CONFIG[\"data_source\"], BASE_CONFIG[\"features\"], BASE_CONFIG[\"target\"])\n",
    "X_train, X_test, y_train, y_test = split_data(\n",
    "    X, y,\n",
    "    test_size=BASE_CONFIG[\"test_size\"],\n",
    "    random_state=BASE_CONFIG[\"random_state\"],\n",
    "    stratify=BASE_CONFIG[\"stratify\"]\n",
    ")\n",
    "# 2. Preprocess Data (do this once based on the training split)\n",
    "X_train_scaled, X_test_scaled, scaler = preprocess_data(X_train, X_test)\n",
    "# (Optional) Save the scaler as an artifact too\n",
    "scaler_filename = os.path.join(BASE_CONFIG[\"artifact_dir\"], f\"scaler_rs_{BASE_CONFIG['random_state']}.joblib\")\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"Saved scaler to {scaler_filename}\")\n",
    "\n",
    "\n",
    "# Loop through each parameter configuration\n",
    "for i, params in enumerate(experiment_params):\n",
    "    run_id = i + 1\n",
    "    print(f\"\\n--- Running Experiment {run_id}/{len(experiment_params)} ---\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "\n",
    "    # 3. Train Model using current params\n",
    "    model = train_model(X_train_scaled, y_train, params)\n",
    "\n",
    "    # 4. Evaluate Model\n",
    "    metrics = evaluate_model(model, X_test_scaled, y_test)\n",
    "    print(f\"Metrics: {metrics}\")\n",
    "\n",
    "    # 5. (Task) Store Parameters and Metrics\n",
    "    run_result = {\n",
    "        \"run_id\": run_id,\n",
    "        **params, # Unpack parameter dictionary\n",
    "        **metrics # Unpack metrics dictionary\n",
    "    }\n",
    "    results_list.append(run_result)\n",
    "\n",
    "    # 6. Artifact Logging: Save the trained model\n",
    "    try:\n",
    "        # Create a descriptive filename\n",
    "        metric_str = f\"acc_{metrics['accuracy']:.3f}\".replace('.', '_') # Make filename safe\n",
    "        param_str = f\"depth_{params.get('max_depth', 'None')}_crit_{params.get('criterion', 'na')}\"\n",
    "        timestamp = int(time.time()) # Add timestamp for uniqueness if needed\n",
    "        model_filename = os.path.join(\n",
    "            BASE_CONFIG[\"artifact_dir\"],\n",
    "            f\"model_run_{run_id}_{param_str}_{metric_str}.joblib\"\n",
    "        )\n",
    "        # Save the model object\n",
    "        joblib.dump(model, model_filename)\n",
    "        print(f\"Saved model artifact to: {model_filename}\")\n",
    "        # Add artifact path to results for tracking\n",
    "        run_result[\"model_artifact\"] = model_filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model for run {run_id}: {e}\")\n",
    "        run_result[\"model_artifact\"] = None # Indicate saving failed\n",
    "\n",
    "print(\"\\n--- Experiment Loop Finished ---\")\n",
    "\n",
    "# --- Result Analysis ---\n",
    "print(\"\\n--- Analyzing Results ---\")\n",
    "\n",
    "# Task: Convert list of results into a Pandas DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Task: Display and sort the DataFrame\n",
    "print(\"Results Summary DataFrame:\")\n",
    "# Display all columns clearly\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000) # Adjust width for better console display\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nResults Sorted by Accuracy (Descending):\")\n",
    "# Sort to find the best performing parameters based on accuracy\n",
    "results_df_sorted = results_df.sort_values(by=\"accuracy\", ascending=False)\n",
    "print(results_df_sorted)\n",
    "\n",
    "# Identify best run parameters\n",
    "if not results_df_sorted.empty:\n",
    "    best_run = results_df_sorted.iloc[0]\n",
    "    print(\"\\n--- Best Performing Run ---\")\n",
    "    print(f\"Run ID: {best_run['run_id']}\")\n",
    "    print(f\"Parameters: {best_run.filter(items=experiment_params[0].keys()).to_dict()}\") # Show only hyperparams\n",
    "    print(f\"Accuracy: {best_run['accuracy']:.4f}\")\n",
    "    print(f\"Model Artifact: {best_run['model_artifact']}\")\n",
    "else:\n",
    "    print(\"\\nNo results to analyze.\")\n",
    "\n",
    "print(\"\\n--- Listing Saved Artifacts ---\")\n",
    "print(f\"Files in '{BASE_CONFIG['artifact_dir']}':\")\n",
    "try:\n",
    "    saved_files = os.listdir(BASE_CONFIG[\"artifact_dir\"])\n",
    "    if saved_files:\n",
    "        for f in saved_files:\n",
    "            print(f\"- {f}\")\n",
    "    else:\n",
    "        print(\"(No files found)\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Directory '{BASE_CONFIG['artifact_dir']}' not found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5a1bf-660e-469c-b6fb-56e3b28f848e",
   "metadata": {},
   "source": [
    "### Are you able to observe the key tasks in code sample C \n",
    "- Explicitly defined parameter sets to try.\n",
    "- Looped through experiments, training, and evaluating for each set.\n",
    "- Manually collected parameters and metrics into a list.\n",
    "- Used Pandas to analyze the collected results.\n",
    "- Manually saved model files (artifacts) with descriptive names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddca58e-420f-4023-897d-3c30131da9fe",
   "metadata": {},
   "source": [
    "## Exericse 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b9ee2a6-8a6a-450b-8a0a-a9ac7c38b359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Simulated Data Versions ---\n",
      "Created 'data_subset_A.csv' (100 samples, scale 1.0)\n",
      "Created 'data_subset_B.csv' (120 samples, scale 1.2)\n",
      "\n",
      "--- Part 1: Impact of Code Changes (Preprocessing Logic) ---\n",
      "\n",
      "Running workflow with preprocess_v1 using data_subset_A...\n",
      "Using preprocess_v1: Scaling feature1 with MinMaxScaler.\n",
      "Using preprocess_v1: Scaling feature1 with MinMaxScaler.\n",
      "Score (S1) with preprocess_v1: 0.9000\n",
      "\n",
      "Running workflow with preprocess_v2 using data_subset_A...\n",
      "Using preprocess_v2: Scaling feature1 & feature2 with StandardScaler.\n",
      "Using preprocess_v2: Scaling feature1 & feature2 with StandardScaler.\n",
      "Score (S2) with preprocess_v2: 0.9667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn') # Suppress simple warnings for clarity\n",
    "\n",
    "# --- Setup: Shared Model Configuration and Dummy Data Generation ---\n",
    "\n",
    "# Fixed model configuration for all experiments in this activity\n",
    "MODEL_PARAMS = {'solver': 'liblinear', 'random_state': 42, 'C': 1.0}\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Generate Dummy Data Subsets\n",
    "def generate_data(seed, num_samples=100, scale_factor=1.0, extra_noise_col=False):\n",
    "    \"\"\"Generates simple reproducible data.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    X = pd.DataFrame({\n",
    "        'feature1': np.random.rand(num_samples) * 10 * scale_factor,\n",
    "        'feature2': np.random.rand(num_samples) * 5 * scale_factor\n",
    "    })\n",
    "    if extra_noise_col:\n",
    "        X['noise_feature'] = np.random.randn(num_samples) * 0.1\n",
    "\n",
    "    # Simple target variable based on feature1\n",
    "    y = (X['feature1'] > (5 * scale_factor)).astype(int)\n",
    "    return X, y\n",
    "\n",
    "# Create two distinct data versions\n",
    "print(\"\\n--- Generating Simulated Data Versions ---\")\n",
    "data_A_X, data_A_y = generate_data(seed=1, num_samples=100, scale_factor=1.0)\n",
    "data_B_X, data_B_y = generate_data(seed=1, num_samples=120, scale_factor=1.2) # More samples, different scale\n",
    "\n",
    "# Save dummy data to CSV (optional, can use DataFrames directly)\n",
    "data_A_X.assign(target=data_A_y).to_csv(\"data_subset_A.csv\", index=False)\n",
    "data_B_X.assign(target=data_B_y).to_csv(\"data_subset_B.csv\", index=False)\n",
    "print(\"Created 'data_subset_A.csv' (100 samples, scale 1.0)\")\n",
    "print(\"Created 'data_subset_B.csv' (120 samples, scale 1.2)\")\n",
    "\n",
    "\n",
    "# --- Part 1: Code Change Impact ---\n",
    "print(\"\\n--- Part 1: Impact of Code Changes (Preprocessing Logic) ---\")\n",
    "\n",
    "# Define preprocessing function v1\n",
    "def preprocess_v1(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Version 1: Scales only feature1 using MinMaxScaler.\"\"\"\n",
    "    print(\"Using preprocess_v1: Scaling feature1 with MinMaxScaler.\")\n",
    "    scaler = MinMaxScaler()\n",
    "    df_processed = df.copy()\n",
    "    df_processed['feature1'] = scaler.fit_transform(df_processed[['feature1']])\n",
    "    # feature2 is untouched\n",
    "    return df_processed\n",
    "\n",
    "# Define preprocessing function v2\n",
    "def preprocess_v2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Version 2: Scales both features using StandardScaler.\"\"\"\n",
    "    print(\"Using preprocess_v2: Scaling feature1 & feature2 with StandardScaler.\")\n",
    "    scaler = StandardScaler()\n",
    "    df_processed = df.copy()\n",
    "    # Apply scaling to all columns present\n",
    "    df_processed[df_processed.columns] = scaler.fit_transform(df_processed)\n",
    "    return df_processed\n",
    "\n",
    "# -- Run with preprocess_v1 --\n",
    "print(\"\\nRunning workflow with preprocess_v1 using data_subset_A...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_A_X, data_A_y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "X_train_p1 = preprocess_v1(X_train)\n",
    "X_test_p1 = preprocess_v1(X_test) # In practice, fit on train, transform test\n",
    "\n",
    "model_v1 = LogisticRegression(**MODEL_PARAMS)\n",
    "model_v1.fit(X_train_p1, y_train)\n",
    "y_pred_v1 = model_v1.predict(X_test_p1)\n",
    "score_S1 = accuracy_score(y_test, y_pred_v1)\n",
    "print(f\"Score (S1) with preprocess_v1: {score_S1:.4f}\")\n",
    "\n",
    "# -- Run with preprocess_v2 --\n",
    "print(\"\\nRunning workflow with preprocess_v2 using data_subset_A...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_A_X, data_A_y, test_size=TEST_SIZE, random_state=RANDOM_STATE) # Re-split same data\n",
    "X_train_p2 = preprocess_v2(X_train)\n",
    "X_test_p2 = preprocess_v2(X_test) # In practice, fit on train, transform test\n",
    "\n",
    "model_v2 = LogisticRegression(**MODEL_PARAMS) # Same model config\n",
    "model_v2.fit(X_train_p2, y_train)\n",
    "y_pred_v2 = model_v2.predict(X_test_p2)\n",
    "score_S2 = accuracy_score(y_test, y_pred_v2)\n",
    "print(f\"Score (S2) with preprocess_v2: {score_S2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b8da4-6fc9-41b9-9f6c-e9fb1f1876f2",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "Notice the scores differ: S1 (v1) vs S2 (v2), even with the same data and model parameters.\n",
    "\n",
    "If you just saw Score S2 months later, how would you know *exactly* which preprocessing logic (v1 or v2) was used?\"\n",
    "\n",
    "How would you revert back to the code that produced S1 if v2 performed worse?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfdfc7cd-7d83-4cef-ac68-ea4fc777051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running workflow with preprocess_v1 using data_subset_A...\n",
      "Using preprocess_v1: Scaling feature1 with MinMaxScaler.\n",
      "Using preprocess_v1: Scaling feature1 with MinMaxScaler.\n",
      "Score (S1_data) with preprocess_v1 and data_A: 0.9000\n",
      "\n",
      "Running workflow with preprocess_v1 using data_subset_B...\n",
      "Using preprocess_v1: Scaling feature1 with MinMaxScaler.\n",
      "Using preprocess_v1: Scaling feature1 with MinMaxScaler.\n",
      "Score (S2_data) with preprocess_v1 and data_B: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# -- Run with data_subset_A --\n",
    "print(\"\\nRunning workflow with preprocess_v1 using data_subset_A...\")\n",
    "# We can reuse the results from Part 1 or recalculate for clarity:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_A_X, data_A_y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "X_train_p1_dataA = preprocess_v1(X_train)\n",
    "X_test_p1_dataA = preprocess_v1(X_test) # Fit on train, transform test ideally\n",
    "model_dataA = LogisticRegression(**MODEL_PARAMS)\n",
    "model_dataA.fit(X_train_p1_dataA, y_train)\n",
    "y_pred_dataA = model_dataA.predict(X_test_p1_dataA)\n",
    "score_S1_data = accuracy_score(y_test, y_pred_dataA)\n",
    "print(f\"Score (S1_data) with preprocess_v1 and data_A: {score_S1_data:.4f}\")\n",
    "\n",
    "# -- Run with data_subset_B --\n",
    "print(\"\\nRunning workflow with preprocess_v1 using data_subset_B...\")\n",
    "# Now use data_B but the SAME preprocessing code (v1) and model params\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_B_X, data_B_y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "X_train_p1_dataB = preprocess_v1(X_train)\n",
    "X_test_p1_dataB = preprocess_v1(X_test) # Fit on train, transform test ideally\n",
    "model_dataB = LogisticRegression(**MODEL_PARAMS) # Same model config\n",
    "model_dataB.fit(X_train_p1_dataB, y_train)\n",
    "y_pred_dataB = model_dataB.predict(X_test_p1_dataB)\n",
    "score_S2_data = accuracy_score(y_test, y_pred_dataB)\n",
    "print(f\"Score (S2_data) with preprocess_v1 and data_B: {score_S2_data:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f3e5f-eca7-4b24-8cc6-60f7cc46a609",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Notice the scores differ: S1_data (Data A) vs S2_data (Data B), even with the same code and model parameters.\n",
    "\n",
    "If you got the result S2_data, how would you know *exactly* which dataset ('data_subset_A.csv' or 'data_subset_B.csv') was used for training?\n",
    "\n",
    "How could you reproduce the S1_data result if 'data_subset_A.csv' was accidentally overwritten or deleted?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3122e3-55ae-498d-988d-d3d813f16863",
   "metadata": {},
   "source": [
    "## Further discussion\n",
    "\n",
    "In Exercise 2, we saved models like 'model_run_4_depth_5_crit_gini_acc_0_933.joblib'.\n",
    "\n",
    "Now imagine finding such a file months later.\n",
    "\n",
    "How can you reliably determine:\n",
    "- Which *exact version* of the preprocessing code (like v1 or v2) was used?\n",
    "- Which *exact version* of the dataset (like data_A or data_B) was used?\n",
    "- Which *exact set* of hyperparameters (already in filename, but need confirmation) was used?\n",
    "\n",
    "Simply having the saved model file is often not enough for reproducibility or debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d392a-95f5-4046-a036-1b3b242f3ea2",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "ML results are highly sensitive to changes in CODE (e.g., preprocessing, model architecture), DATA (e.g., samples, features, distribution), and PARAMETERS.\n",
    "Without robust versioning of all these components, it's extremely difficult to:\n",
    "- Reproduce past results.\n",
    "- Understand why performance changed.\n",
    "- Debug issues reliably.\n",
    "- Collaborate effectively.\n",
    "This simulation motivates the use of dedicated version control tools (like Git for code, DVC for data) and experiment tracking platforms (like MLflow) in MLOps workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
